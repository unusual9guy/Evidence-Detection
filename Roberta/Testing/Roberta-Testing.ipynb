{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### File for predicting the results of the trained model"
      ],
      "metadata": {
        "id": "gBp2IefJwHT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author :  Vansh Goenka"
      ],
      "metadata": {
        "id": "QW0MHJN5wHV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "pMB5SBKxwHYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# Download the stop words corpus if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "# os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "%env TF_USE_LEGACY_KERAS=1\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lVU8cBkQRYPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f65f642-6930-4b67-a669-3fa195b2eb2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TF_USE_LEGACY_KERAS=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up the dataframes and model"
      ],
      "metadata": {
        "id": "deIoZ1k8x0I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data from CSV\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "trial_data = pd.read_csv('/content/drive/My Drive/ED_trial.csv')\n",
        "train_data = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "dev_data = pd.read_csv('/content/drive/My Drive/dev.csv')\n",
        "eval_data = pd.read_csv('/content/drive/My Drive/eval.csv')\n",
        "dev_data['Evidence'] = dev_data['Evidence'].astype(str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oh2LvPnqRuv",
        "outputId": "f230a987-e029-4bdc-95a5-f5a524e09dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-processing the data"
      ],
      "metadata": {
        "id": "jfiC10GZzCyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "model_roberta = tf.keras.models.load_model('/content//drive/My Drive/RoBERTa_model.h5', custom_objects={\"TFRobertaModel\": transformers.TFRobertaModel})"
      ],
      "metadata": {
        "id": "J61XzVZbzC94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pBmuComATECK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to evluate the results using the same metrics as EvalAI- check if its needed"
      ],
      "metadata": {
        "id": "2PHGG4i7yUk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation function to measure model performance\n",
        "def evaluate(actual, predicted):\n",
        "    accuracy = accuracy_score(actual, predicted)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    report = classification_report(actual, predicted)\n",
        "    print(f\"Classification report: {report}\")\n",
        "    f1_macro = f1_score(actual, predicted, average='macro')\n",
        "    print(f\"F1 score - macro: {f1_macro}\")\n",
        "    precision_macro = precision_score(actual, predicted, average='macro')\n",
        "    print(f\"Precision - macro: {precision_macro}\")\n",
        "    recall_macro = recall_score(actual, predicted, average='macro')\n",
        "    print(f\"Recall - macro: {recall_macro}\")\n",
        "    f1_weighted = f1_score(actual, predicted, average='weighted')\n",
        "    print(f\"F1 score (weighted): {f1_weighted}\")\n",
        "    precision_weighted = precision_score(actual, predicted, average='weighted')\n",
        "    print(f\"Precision (weighted): {precision_weighted}\")\n",
        "    recall_weighted = recall_score(actual, predicted, average='weighted')\n",
        "    print(f\"Recall (weighted): {recall_weighted}\")\n",
        "    labels = [0, 1]\n",
        "    cm = confusion_matrix(actual, predicted)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='g', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OYUQ-yqlqVSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the models"
      ],
      "metadata": {
        "id": "a9hUeM4Mzrhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using RoBERTa"
      ],
      "metadata": {
        "id": "NxcqQ7v9zuWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # Create a new folder called \"Models\" in /content/resources/\n",
        "# !mkdir -p /content/resources/Models\n",
        "\n",
        "# # Download the models from google drive into the \"Models\" folder\n",
        "# !gdown --output \"/content/resources/Models/t5_model.h5\" 1cF2SIFknn-DVoYo2xjKP4raSmLAlyvIU\n",
        "# !gdown --output \"/content/resources/Models/t5_flan_model.h5\" 1-D7BhUqyX0kDixryv6WbDv5HX2kPnbbs\n",
        "# !gdown --output \"/content/resources/Models/roberta_model.h5\" 1-L4AsV0kLzZ49DWK9bYq4OTslVc8y1_P"
      ],
      "metadata": {
        "id": "w3VvSQQmqYMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load models using transformers"
      ],
      "metadata": {
        "id": "NU9ImQ90zyHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# #Load models as transformers\n",
        "# modelT5 = tf.keras.models.load_model('/content/resources/Models/t5_model.h5', custom_objects={\"TFT5EncoderModel\": transformers.TFT5EncoderModel})\n",
        "# modelRoBERTa = tf.keras.models.load_model('/content/resources/Models/roberta_model.h5', custom_objects={\"TFRobertaModel\": transformers.TFRobertaModel})\n",
        "# modelFlanT5 = tf.keras.models.load_model('/content/resources/Models/t5_flan_model.h5', custom_objects={\"TFT5EncoderModel\": transformers.TFT5EncoderModel})\n"
      ],
      "metadata": {
        "id": "Org-ACRgqbiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing predicted labels to csv\n",
        "Generating Dataframe"
      ],
      "metadata": {
        "id": "tojTRimVz1ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pd.set_option('display.max_rows', None)\n",
        "# result_df = pd.DataFrame({'prediction': ensemble_predictions})\n",
        "# column_name_row = pd.DataFrame({'prediction': ['prediction']}, index=[0])\n",
        "# result_df['prediction'] = result_df['prediction'].astype(int)\n",
        "# result_df = pd.concat([column_name_row, result_df]).reset_index(drop=True)\n",
        "# result_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "U4kagXtLqqq5",
        "outputId": "a135e186-9046-4624-ba32-f8ad3bde2ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'choice' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-90d24bfd462b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Apply augmentation to Claim text in training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Claim'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_with_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Claim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-c81a75da2355>\u001b[0m in \u001b[0;36maugment_with_synonyms\u001b[0;34m(text, percentage)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msynonyms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mnew_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mnew_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'choice' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import numpy as np\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def roberta_encode(claims, evidences, tokenizer, max_length=120):\n",
        "    concatenated_inputs = [c + '  ' + e for c, e in zip(np.array(evidences), np.array(claims))]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        concatenated_inputs,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask']\n",
        "    }\n",
        "\n",
        "train_input_Roberta = roberta_encode(train_data['Claim'].values, train_data['Evidence'].values, tokenizer)\n",
        "dev_input_Roberta = roberta_encode(dev_data['Claim'].values, dev_data['Evidence'].values, tokenizer)\n",
        "trial_input_Roberta = roberta_encode(trial_data['Claim'].values, trial_data['Evidence'].values, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kpM75LdFTQ5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qqGdgltxz31j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictionsRoBERTa = np.argmax(model_roberta.predict(trial_input_Roberta), axis=1)"
      ],
      "metadata": {
        "id": "QG_he2U4z36h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e460966-29d0-4cdc-9e66-9508496d6806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 28s 7s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "acc = np.mean(predictionsRoBERTa == trial_data.label.values)\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "Z423EnxuTrws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48fd74b-a2d4-402c-fae7-482169d17d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.96\n"
          ]
        }
      ]
    }
  ]
}